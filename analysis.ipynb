{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 requests pandas --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "data = {\n",
    "    \"Company\": [],\n",
    "    \"Website\": [],\n",
    "    \"Relevant\": [],\n",
    "    \"Category\": [],\n",
    "    \"Manufacturer\": [],\n",
    "    \"Brand\": [],\n",
    "    \"Distributor\": [],\n",
    "    \"F&B\": [],\n",
    "    \"Probiotics\": [],\n",
    "    \"Fortification\": [],\n",
    "    \"Gut Health\": [],\n",
    "    \"Womens Health\": [],\n",
    "    \"Cognitive Health\": []\n",
    "}\n",
    "\n",
    "\n",
    "companies = [\n",
    "    {\"name\": \"Namaste India\", \"url\": \"https://www.namasteindiafoods.com/milk.html\"},\n",
    "    {\"name\": \"AB Biotics\", \"url\": \"https://www.ab-biotics.com/\"},\n",
    "    {\"name\": \"Aliga Microalgae Company\", \"url\": \"https://www.aliga.dk/\"},\n",
    "    {\"name\": \"Zinereo Pharma\", \"url\": \"https://www.zinereopharma.com/en/\"},\n",
    "    {\"name\": \"Nutravends\", \"url\": \"https://nutravends.com/\"}\n",
    "]\n",
    "\n",
    "def scrape_website(company):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(company[\"url\"], headers=headers)\n",
    "        response.raise_for_status()  #  for request errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        f_and_b = \"Yes\" if \"food\" in soup.text.lower() and \"beverage\" in soup.text.lower() else \"No\"\n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "\n",
    "        category = \"N/A\"\n",
    "        if \"manufacturer\" in soup.text.lower():\n",
    "            category = \"Bulk (Manufacturer)\"\n",
    "        elif \"distributor\" in soup.text.lower():\n",
    "            category = \"Bulk (Distributor)\"\n",
    "        elif \"brand\" in soup.text.lower():\n",
    "            category = \"Brand\"\n",
    "        else:\n",
    "            category = \"F&B\"\n",
    "\n",
    "        data[\"Company\"].append(company[\"name\"])\n",
    "        data[\"Website\"].append(company[\"url\"])\n",
    "        data[\"Relevant\"].append(\"Yes\")\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Manufacturer\"].append(manufacturer)\n",
    "        data[\"Brand\"].append(brand)\n",
    "        data[\"Distributor\"].append(distributor)\n",
    "        data[\"F&B\"].append(f_and_b)\n",
    "        data[\"Probiotics\"].append(probiotics)\n",
    "        data[\"Fortification\"].append(fortification)\n",
    "        data[\"Gut Health\"].append(gut_health)\n",
    "        data[\"Womens Health\"].append(womens_health)\n",
    "        data[\"Cognitive Health\"].append(cognitive_health)\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error processing {company['name']}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company['name']}: {e}\")\n",
    "\n",
    "\n",
    "for company in companies:\n",
    "    scrape_website(company)\n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "file_path = \"company_data.csv\"\n",
    "if os.path.exists(file_path):\n",
    "\n",
    "    df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Data scraping completed. Results saved to 'company_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies fetched: 1\n",
      "Processing Nestle...\n",
      "Accessing URL: http://www.nestle.com\n",
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from io import StringIO \n",
    "\n",
    "# function to fetch from a public Google Sheets document\n",
    "def fetch_companies_from_public_sheet(sheet_url):\n",
    "    csv_url = sheet_url.replace('/edit?usp=sharing', '/gviz/tq?tqx=out:csv')\n",
    "\n",
    "    response = requests.get(csv_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "   \n",
    "        data = pd.read_csv(StringIO(response.content.decode('utf-8')))\n",
    "        \n",
    "     \n",
    "        data.columns = data.columns.str.strip().str.replace('\"', '')  # Clean column names\n",
    "        data = data[['Company', 'Website']].dropna()  # Ensure we only get companies and websites\n",
    "        \n",
    "        companies = []\n",
    "        for index, row in data.iterrows():\n",
    "            company_info = {\n",
    "                \"name\": row['Company'].strip(),\n",
    "                \"url\": row['Website'].strip()\n",
    "            }\n",
    "            companies.append(company_info)\n",
    "\n",
    "        return companies\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    try:\n",
    "        print(f\"Accessing URL: {url}\")  \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "       \n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        relevant = \"Yes\"  \n",
    "        category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "        \n",
    "     \n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "        \n",
    "        return {\n",
    "            \"Company\": company[\"name\"],\n",
    "            \"Website\": url,\n",
    "            \"Relevant\": relevant,\n",
    "            \"Category\": category,\n",
    "            \"Manufacturer\": manufacturer,\n",
    "            \"Brand\": brand,\n",
    "            \"Distributor\": distributor,\n",
    "            \"F&B\": \"Yes\",  \n",
    "            \"Probiotics\": probiotics,\n",
    "            \"Fortification\": fortification,\n",
    "            \"Gut Health\": gut_health,\n",
    "            \"Womens Health\": womens_health,\n",
    "            \"Cognitive Health\": cognitive_health\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {company['name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1RESyISt077FgGm9tz-UZyYSu4M_0mVCvGBCLhmzN-p8/edit?usp=sharing\"\n",
    "\n",
    "\n",
    "companies = fetch_companies_from_public_sheet(sheet_url)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Number of companies fetched: {len(companies)}\")  \n",
    "\n",
    "\n",
    "for company in companies:\n",
    "    print(f\"Processing {company['name']}...\")  \n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "csv_file = 'company_data.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df.to_csv(csv_file, mode='a', header=False, index=False)  # Append mode\n",
    "else:\n",
    "    df.to_csv(csv_file, index=False)  # Write mode\n",
    "    print(f\"Column headers added to '{csv_file}'.\")\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n",
    "\n",
    "def fetch_companies_from_public_sheet(sheet_url):\n",
    "    # converting the Google Sheets URL to a downloadable CSV format\n",
    "    csv_url = sheet_url.replace('/edit?usp=sharing', '/gviz/tq?tqx=out:csv')\n",
    "    \n",
    "    # Fetch the CSV data\n",
    "    response = requests.get(csv_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Read the CSV data into a DataFrame\n",
    "        data = pd.read_csv(StringIO(response.content.decode('utf-8')))\n",
    "        \n",
    "        \n",
    "        data.columns = data.columns.str.strip().str.replace('\"', '')  \n",
    "        data = data[['Company', 'Website']].dropna()  \n",
    "        \n",
    "        \n",
    "        print(f\"Number of rows in fetched data: {len(data)}\")  # Number of rows fetched\n",
    "        print(data.head())  # Print first few rows for verification\n",
    "        \n",
    "        companies = []\n",
    "        for index, row in data.iterrows():\n",
    "            company_info = {\n",
    "                \"name\": row['Company'].strip(),\n",
    "                \"url\": row['Website'].strip()\n",
    "            }\n",
    "            companies.append(company_info)\n",
    "\n",
    "        return companies\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in fetched data: 1\n",
      "  Company                Website\n",
      "0  Nestle  http://www.nestle.com\n",
      "Number of companies fetched: 1\n",
      "Processing Nestle...\n",
      "Accessing URL: http://www.nestle.com\n",
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "def fetch_companies_from_public_sheet(sheet_url):\n",
    "    csv_url = sheet_url.replace('/edit?usp=sharing', '/gviz/tq?tqx=out:csv')\n",
    "    response = requests.get(csv_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = pd.read_csv(StringIO(response.content.decode('utf-8')))\n",
    "        data.columns = data.columns.str.strip().str.replace('\"', '')\n",
    "        data = data[['Company', 'Website']].dropna()\n",
    "        \n",
    "        print(f\"Number of rows in fetched data: {len(data)}\")\n",
    "        print(data.head())\n",
    "        \n",
    "        companies = []\n",
    "        for index, row in data.iterrows():\n",
    "            company_info = {\n",
    "                \"name\": row['Company'].strip(),\n",
    "                \"url\": row['Website'].strip()\n",
    "            }\n",
    "            companies.append(company_info)\n",
    "\n",
    "        return companies\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    try:\n",
    "        print(f\"Accessing URL: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        relevant = \"Yes\"\n",
    "        category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "        \n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "        \n",
    "        return {\n",
    "            \"Company\": company[\"name\"],\n",
    "            \"Website\": url,\n",
    "            \"Relevant\": relevant,\n",
    "            \"Category\": category,\n",
    "            \"Manufacturer\": manufacturer,\n",
    "            \"Brand\": brand,\n",
    "            \"Distributor\": distributor,\n",
    "            \"F&B\": \"Yes\",\n",
    "            \"Probiotics\": probiotics,\n",
    "            \"Fortification\": fortification,\n",
    "            \"Gut Health\": gut_health,\n",
    "            \"Womens Health\": womens_health,\n",
    "            \"Cognitive Health\": cognitive_health\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {company['name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1RESyISt077FgGm9tz-UZyYSu4M_0mVCvGBCLhmzN-p8/edit?usp=sharing\"\n",
    "\n",
    "companies = fetch_companies_from_public_sheet(sheet_url)\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Number of companies fetched: {len(companies)}\")\n",
    "\n",
    "for company in companies:\n",
    "    print(f\"Processing {company['name']}...\")\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "csv_file = 'company_data.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Column headers added to '{csv_file}'.\")\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_companies_from_public_sheet(sheet_url):\n",
    "    csv_url = sheet_url.replace('/edit?usp=sharing', '/gviz/tq?tqx=out:csv')\n",
    "    response = requests.get(csv_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        raw_data = response.content.decode('utf-8')\n",
    "        print(\"Raw CSV data:\\n\", raw_data)\n",
    "        data = pd.read_csv(StringIO(raw_data))\n",
    "        data.columns = data.columns.str.strip().str.replace('\"', '')\n",
    "        data = data[['Company', 'Website']].dropna()\n",
    "        \n",
    "        print(f\"Number of rows in fetched data: {len(data)}\")\n",
    "        print(data.head())\n",
    "\n",
    "        if len(data) == 0:\n",
    "            print(\"No data available after cleaning.\")\n",
    "            return []\n",
    "\n",
    "        companies = []\n",
    "        for index, row in data.iterrows():\n",
    "            company_info = {\n",
    "                \"name\": row['Company'].strip(),\n",
    "                \"url\": row['Website'].strip()\n",
    "            }\n",
    "            companies.append(company_info)\n",
    "\n",
    "        print(f\"Fetched companies: {companies}\")\n",
    "        return companies\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Company\",\"Website\",\"Relevant\",\"Category\",\"\",\"Manufacturer\",\"Brand\",\"Distributor\",\"F&B\",\"\",\"Probiotics\",\"Fortification\",\"\",\"Gut Health\",\"Womens Health\",\"Cognitive Health\"\n",
      "\"Nestle\",\"http://www.nestle.com\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
      "Raw CSV data:\n",
      " \"Company\",\"Website\",\"Relevant\",\"Category\",\"\",\"Manufacturer\",\"Brand\",\"Distributor\",\"F&B\",\"\",\"Probiotics\",\"Fortification\",\"\",\"Gut Health\",\"Womens Health\",\"Cognitive Health\"\n",
      "\"Nestle\",\"http://www.nestle.com\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
      "Number of rows in fetched data: 1\n",
      "  Company                Website\n",
      "0  Nestle  http://www.nestle.com\n",
      "Fetched companies: [{'name': 'Nestle', 'url': 'http://www.nestle.com'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Nestle', 'url': 'http://www.nestle.com'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_companies_from_public_sheet(\"https://docs.google.com/spreadsheets/d/1RESyISt077FgGm9tz-UZyYSu4M_0mVCvGBCLhmzN-p8/edit?usp=sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company': 'Nestle', 'Website': 'http://www.nestle.com', 'Relevant': 'Yes', 'Category': 'Brand', 'Manufacturer': 'No', 'Brand': 'Yes', 'Distributor': 'No', 'F&B': 'Yes', 'Probiotics': 'No', 'Fortification': 'No', 'Gut Health': 'No', 'Womens Health': 'No', 'Cognitive Health': 'No'}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    try:\n",
    "        print(f\"Accessing URL: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        relevant = \"Yes\"\n",
    "        category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "        \n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "        \n",
    "        return {\n",
    "            \"Company\": company[\"name\"],\n",
    "            \"Website\": url,\n",
    "            \"Relevant\": relevant,\n",
    "            \"Category\": category,\n",
    "            \"Manufacturer\": manufacturer,\n",
    "            \"Brand\": brand,\n",
    "            \"Distributor\": distributor,\n",
    "            \"F&B\": \"Yes\",\n",
    "            \"Probiotics\": probiotics,\n",
    "            \"Fortification\": fortification,\n",
    "            \"Gut Health\": gut_health,\n",
    "            \"Womens Health\": womens_health,\n",
    "            \"Cognitive Health\": cognitive_health\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {company['name']} ({url}): {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1RESyISt077FgGm9tz-UZyYSu4M_0mVCvGBCLhmzN-p8/edit?usp=sharing\"\n",
    "\n",
    "companies = fetch_companies_from_public_sheet(sheet_url)\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Number of companies fetched: {len(companies)}\")\n",
    "\n",
    "for company in companies:\n",
    "    print(f\"Processing {company['name']}...\")\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "csv_file = 'company_data.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Column headers added to '{csv_file}'.\")\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH -2 : Trying to Continue without google spreadsheet, having a local dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                company_name                            url\n",
      "0                     Nestle                 www.nestle.com\n",
      "1   Dr. Reddy's Laboratories               www.drreddys.com\n",
      "2                       Coca       www.coca-colacompany.com\n",
      "3                     Pfizer                 www.pfizer.com\n",
      "4                    PepsiCo                www.pepsico.com\n",
      "5          Johnson & Johnson                    www.jnj.com\n",
      "6                     Danone                 www.danone.com\n",
      "7                      Bayer                  www.bayer.com\n",
      "8              General Mills           www.generalmills.com\n",
      "9      GlaxoSmithKline (GSK)                    www.gsk.com\n",
      "10                 Kelloggs               www.kelloggs.com\n",
      "11               Merck & Co.                  www.merck.com\n",
      "12                  Unilever               www.unilever.com\n",
      "13                     Roche                  www.roche.com\n",
      "14             Nestle Waters           www.nestlewaters.com\n",
      "15                    Sanofi                 www.sanofi.com\n",
      "16    Mondelez International  www.mondelezinternational.com\n",
      "17                  Novartis               www.novartis.com\n",
      "18               Kraft Heinz      www.kraftheinzcompany.com\n",
      "19     Eli Lilly and Company                  www.lilly.com\n",
      "20               Tyson Foods             www.tysonfoods.com\n",
      "21      Teva Pharmaceuticals              www.tevapharm.com\n",
      "22        Mars, Incorporated                   www.mars.com\n",
      "23                    AbbVie                 www.abbvie.com\n",
      "24     Campbell Soup Company    www.campbellsoupcompany.com\n",
      "25                     Amgen                  www.amgen.com\n",
      "26            Conagra Brands          www.conagrabrands.com\n",
      "27               AstraZeneca            www.astrazeneca.com\n",
      "28              Molson Coors            www.molsoncoors.com\n",
      "29      Boehringer Ingelheim    www.boehringeringelheim.com\n",
      "30                  AB InBev                www.abinbev.com\n",
      "31                      BASF                   www.basf.com\n",
      "32                    Diageo                 www.diageo.com\n",
      "33    Procter & Gamble (P&G)                     www.pg.com\n",
      "34                  Heineken     www.theheinekencompany.com\n",
      "35                 Medtronic              www.medtronic.com\n",
      "36                  McKesson               www.mckesson.com\n",
      "37         AmerisourceBergen      www.amerisourcebergen.com\n",
      "38           Cardinal Health         www.cardinalhealth.com\n",
      "39        Medline Industries                www.medline.com\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# using different encoding instead of utf-8\n",
    "try:\n",
    "    dataset = pd.read_csv(\"dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "    #cleaning th dataa\n",
    "    dataset = dataset.drop(columns=['Unnamed: 2'], errors='ignore')\n",
    "    print(dataset)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nestle...\n",
      "Accessing URL: http://www.nestle.com\n",
      "Processing Dr. Reddy's Laboratories...\n",
      "Accessing URL: http://www.drreddys.com\n",
      "Processing Coca...\n",
      "Accessing URL: http://www.coca-colacompany.com\n",
      "Processing Pfizer...\n",
      "Accessing URL: http://www.pfizer.com\n",
      "Processing PepsiCo...\n",
      "Accessing URL: http://www.pepsico.com\n",
      "Error processing PepsiCo: 403 Client Error: Forbidden for url: https://www.pepsico.com/\n",
      "Processing Johnson & Johnson...\n",
      "Accessing URL: http://www.jnj.com\n",
      "Processing Danone...\n",
      "Accessing URL: http://www.danone.com\n",
      "Processing Bayer...\n",
      "Accessing URL: http://www.bayer.com\n",
      "Error processing Bayer: 403 Client Error: Forbidden for url: http://www.bayer.com/\n",
      "Processing General Mills...\n",
      "Accessing URL: http://www.generalmills.com\n",
      "Processing GlaxoSmithKline (GSK)...\n",
      "Accessing URL: http://www.gsk.com\n",
      "Processing Kelloggs...\n",
      "Accessing URL: http://www.kelloggs.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "dataset = dataset.drop(columns=['Unnamed: 2'], errors='ignore')\n",
    "\n",
    "def ensure_url_scheme(df, url_column):\n",
    "    for index, row in df.iterrows():\n",
    "        url = row[url_column]\n",
    "        if not url.startswith('http://'):\n",
    "            url = 'http://' + url\n",
    "        row[url_column] = url\n",
    "\n",
    "ensure_url_scheme(dataset, 'url')\n",
    "\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    try:\n",
    "        print(f\"Accessing URL: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        relevant = \"Yes\"\n",
    "        category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "        \n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "        \n",
    "        return {\n",
    "            \"Company\": company[\"company_name\"],\n",
    "            \"Website\": url,\n",
    "            \"Relevant\": relevant,\n",
    "            \"Category\": category,\n",
    "            \"Manufacturer\": manufacturer,\n",
    "            \"Brand\": brand,\n",
    "            \"Distributor\": distributor,\n",
    "            \"F&B\": \"Yes\",\n",
    "            \"Probiotics\": probiotics,\n",
    "            \"Fortification\": fortification,\n",
    "            \"Gut Health\": gut_health,\n",
    "            \"Womens Health\": womens_health,\n",
    "            \"Cognitive Health\": cognitive_health\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing {company['company_name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, company in dataset.iterrows():\n",
    "    print(f\"Processing {company['company_name']}...\")\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "csv_file = 'company_data.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 24, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 2\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to ensure URLs have the 'http://' scheme\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhupe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhupe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhupe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\bhupe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 24, saw 3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "dataset = dataset.drop(columns=['Unnamed: 2'], errors='ignore')\n",
    "\n",
    "def ensure_url_scheme(df, url_column):\n",
    "    for index, row in df.iterrows():\n",
    "        url = row[url_column]\n",
    "        if not (url.startswith('http://') or url.startswith('https://')):\n",
    "            url = 'http://' + url\n",
    "        row[url_column] = url\n",
    "\n",
    "ensure_url_scheme(dataset, 'url')\n",
    "\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    attempts = 3\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            print(f\"Accessing URL: {url} (Attempt {attempt + 1})\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "            brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "            distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "            relevant = \"Yes\"\n",
    "            category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "            \n",
    "            probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "            fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "            gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "            womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "            cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "            \n",
    "            return {\n",
    "                \"Company\": company[\"company_name\"],\n",
    "                \"Website\": url,\n",
    "                \"Relevant\": relevant,\n",
    "                \"Category\": category,\n",
    "                \"Manufacturer\": manufacturer,\n",
    "                \"Brand\": brand,\n",
    "                \"Distributor\": distributor,\n",
    "                \"F&B\": \"Yes\",\n",
    "                \"Probiotics\": probiotics,\n",
    "                \"Fortification\": fortification,\n",
    "                \"Gut Health\": gut_health,\n",
    "                \"Womens Health\": womens_health,\n",
    "                \"Cognitive Health\": cognitive_health\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 403:\n",
    "                print(f\"Access denied for {company['company_name']}: {e}. Retrying...\")\n",
    "            else:\n",
    "                print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, company in dataset.iterrows():\n",
    "    print(f\"Processing {company['company_name']}...\")\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "csv_file = 'company_data.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nestle...\n",
      "Accessing URL: http://www.nestle.com (Attempt 1)\n",
      "Processing Dr. Reddy's Laboratories...\n",
      "Accessing URL: http://www.drreddys.com (Attempt 1)\n",
      "Processing Coca...\n",
      "Accessing URL: http://www.coca-colacompany.com (Attempt 1)\n",
      "Processing Pfizer...\n",
      "Accessing URL: http://www.pfizer.com (Attempt 1)\n",
      "Processing PepsiCo...\n",
      "Accessing URL: http://www.pepsico.com (Attempt 1)\n",
      "Processing Johnson & Johnson...\n",
      "Accessing URL: http://www.jnj.com (Attempt 1)\n",
      "Processing Danone...\n",
      "Accessing URL: http://www.danone.com (Attempt 1)\n",
      "Processing Bayer...\n",
      "Accessing URL: http://www.bayer.com (Attempt 1)\n",
      "Access denied for Bayer: 403 Client Error: Forbidden for url: https://www.bayer.com/. Retrying...\n",
      "Accessing URL: http://www.bayer.com (Attempt 2)\n",
      "Access denied for Bayer: 403 Client Error: Forbidden for url: https://www.bayer.com/. Retrying...\n",
      "Accessing URL: http://www.bayer.com (Attempt 3)\n",
      "Access denied for Bayer: 403 Client Error: Forbidden for url: https://www.bayer.com/. Retrying...\n",
      "Processing General Mills...\n",
      "Accessing URL: http://www.generalmills.com (Attempt 1)\n",
      "Processing GlaxoSmithKline (GSK)...\n",
      "Accessing URL: http://www.gsk.com (Attempt 1)\n",
      "Processing Merck & Co....\n",
      "Accessing URL: http://www.merck.com (Attempt 1)\n",
      "Processing Unilever...\n",
      "Accessing URL: http://www.unilever.com (Attempt 1)\n",
      "Processing Roche...\n",
      "Accessing URL: http://www.roche.com (Attempt 1)\n",
      "Processing Nestle Waters...\n",
      "Accessing URL: http://www.nestlewaters.com (Attempt 1)\n",
      "Processing Sanofi...\n",
      "Accessing URL: http://www.sanofi.com (Attempt 1)\n",
      "Processing Mondelez International...\n",
      "Accessing URL: http://www.mondelezinternational.com (Attempt 1)\n",
      "Processing Novartis...\n",
      "Accessing URL: http://www.novartis.com (Attempt 1)\n",
      "Processing Kraft Heinz...\n",
      "Accessing URL: http://www.kraftheinzcompany.com (Attempt 1)\n",
      "Processing Eli Lilly and Company...\n",
      "Accessing URL: http://www.lilly.com (Attempt 1)\n",
      "Access denied for Eli Lilly and Company: 403 Client Error: Forbidden for url: https://www.lilly.com/. Retrying...\n",
      "Accessing URL: http://www.lilly.com (Attempt 2)\n",
      "Access denied for Eli Lilly and Company: 403 Client Error: Forbidden for url: https://www.lilly.com/. Retrying...\n",
      "Accessing URL: http://www.lilly.com (Attempt 3)\n",
      "Access denied for Eli Lilly and Company: 403 Client Error: Forbidden for url: https://www.lilly.com/. Retrying...\n",
      "Processing Tyson Foods...\n",
      "Accessing URL: http://www.tysonfoods.com (Attempt 1)\n",
      "Processing Teva Pharmaceuticals...\n",
      "Accessing URL: http://www.tevapharm.com (Attempt 1)\n",
      "Processing Mars, Incorporated...\n",
      "Accessing URL: http://www.mars.com (Attempt 1)\n",
      "Processing AbbVie...\n",
      "Accessing URL: http://www.abbvie.com (Attempt 1)\n",
      "Access denied for AbbVie: 403 Client Error: Forbidden for url: https://www.abbvie.com/. Retrying...\n",
      "Accessing URL: http://www.abbvie.com (Attempt 2)\n",
      "Access denied for AbbVie: 403 Client Error: Forbidden for url: https://www.abbvie.com/. Retrying...\n",
      "Accessing URL: http://www.abbvie.com (Attempt 3)\n",
      "Access denied for AbbVie: 403 Client Error: Forbidden for url: https://www.abbvie.com/. Retrying...\n",
      "Processing Campbell Soup Company...\n",
      "Accessing URL: http://www.campbellsoupcompany.com (Attempt 1)\n",
      "Processing Amgen...\n",
      "Accessing URL: http://www.amgen.com (Attempt 1)\n",
      "Processing Conagra Brands...\n",
      "Accessing URL: http://www.conagrabrands.com (Attempt 1)\n",
      "Processing AstraZeneca...\n",
      "Accessing URL: http://www.astrazeneca.com (Attempt 1)\n",
      "Processing Molson Coors...\n",
      "Accessing URL: http://www.molsoncoors.com (Attempt 1)\n",
      "Processing Boehringer Ingelheim...\n",
      "Accessing URL: http://www.boehringeringelheim.com (Attempt 1)\n",
      "Processing AB InBev...\n",
      "Accessing URL: http://www.abinbev.com (Attempt 1)\n",
      "Processing BASF...\n",
      "Accessing URL: http://www.basf.com (Attempt 1)\n",
      "Processing Diageo...\n",
      "Accessing URL: http://www.diageo.com (Attempt 1)\n",
      "Processing Procter & Gamble (P&G)...\n",
      "Accessing URL: http://www.pg.com (Attempt 1)\n",
      "Processing Heineken...\n",
      "Accessing URL: http://www.theheinekencompany.com (Attempt 1)\n",
      "Processing Medtronic...\n",
      "Accessing URL: http://www.medtronic.com (Attempt 1)\n",
      "Processing McKesson...\n",
      "Accessing URL: http://www.mckesson.com (Attempt 1)\n",
      "Processing AmerisourceBergen...\n",
      "Accessing URL: http://www.amerisourcebergen.com (Attempt 1)\n",
      "Processing Cardinal Health...\n",
      "Accessing URL: http://www.cardinalhealth.com (Attempt 1)\n",
      "Processing Medline Industries...\n",
      "Accessing URL: http://www.medline.com (Attempt 1)\n",
      "Access denied for Medline Industries: 403 Client Error: Forbidden for url: https://www.medline.com/. Retrying...\n",
      "Accessing URL: http://www.medline.com (Attempt 2)\n",
      "Access denied for Medline Industries: 403 Client Error: Forbidden for url: https://www.medline.com/. Retrying...\n",
      "Accessing URL: http://www.medline.com (Attempt 3)\n",
      "Access denied for Medline Industries: 403 Client Error: Forbidden for url: https://www.medline.com/. Retrying...\n",
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\", encoding=\"ISO-8859-1\", on_bad_lines='skip')\n",
    "dataset = dataset.drop(columns=['Unnamed: 2'], errors='ignore')\n",
    "\n",
    "def ensure_url_scheme(df, url_column):\n",
    "    for index, row in df.iterrows():\n",
    "        url = row[url_column]\n",
    "        if not url.startswith('http://'):\n",
    "            url = 'http://' + url\n",
    "        row[url_column] = url\n",
    "\n",
    "ensure_url_scheme(dataset, 'url')\n",
    "\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    attempts = 3\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            print(f\"Accessing URL: {url} (Attempt {attempt + 1})\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "            brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "            distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "            relevant = \"Yes\"\n",
    "            category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "            probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "            fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "            gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "            womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "            cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "            \n",
    "            return {\n",
    "                \"Company\": company[\"company_name\"],\n",
    "                \"Website\": url,\n",
    "                \"Relevant\": relevant,\n",
    "                \"Category\": category,\n",
    "                \"Manufacturer\": manufacturer,\n",
    "                \"Brand\": brand,\n",
    "                \"Distributor\": distributor,\n",
    "                \"F&B\": \"Yes\",\n",
    "                \"Probiotics\": probiotics,\n",
    "                \"Fortification\": fortification,\n",
    "                \"Gut Health\": gut_health,\n",
    "                \"Womens Health\": womens_health,\n",
    "                \"Cognitive Health\": cognitive_health\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 403:\n",
    "                print(f\"Access denied for {company['company_name']}: {e}. Retrying...\")\n",
    "            else:\n",
    "                print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, company in dataset.iterrows():\n",
    "    print(f\"Processing {company['company_name']}...\")\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "csv_file = 'company_data.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "data = {\n",
    "    \"Company\": [],\n",
    "    \"Website\": [],\n",
    "    \"Relevant\": [],\n",
    "    \"Category\": [],\n",
    "    \"Manufacturer\": [],\n",
    "    \"Brand\": [],\n",
    "    \"Distributor\": [],\n",
    "    \"F&B\": [],\n",
    "    \"Probiotics\": [],\n",
    "    \"Fortification\": [],\n",
    "    \"Gut Health\": [],\n",
    "    \"Womens Health\": [],\n",
    "    \"Cognitive Health\": []\n",
    "}\n",
    "\n",
    "companies = [\n",
    "    {'name': 'bayer', 'url': 'https://www.bayer.com'},\n",
    "    {'name': 'lilly', 'url': 'https://www.lilly.com'},\n",
    "    {'name': 'abbvie', 'url': 'https://www.abbvie.com'},\n",
    "    {'name': 'medline', 'url': 'https://www.medline.com'},\n",
    "]\n",
    "\n",
    "def scrape_website(company):\n",
    "    try:\n",
    "        driver.get(company[\"url\"])\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        f_and_b = \"Yes\" if \"food\" in soup.text.lower() and \"beverage\" in soup.text.lower() else \"No\"\n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "\n",
    "        category = \"N/A\"\n",
    "        if manufacturer == \"Yes\":\n",
    "            category = \"Bulk (Manufacturer)\"\n",
    "        elif distributor == \"Yes\":\n",
    "            category = \"Bulk (Distributor)\"\n",
    "        elif brand == \"Yes\":\n",
    "            category = \"Brand\"\n",
    "        else:\n",
    "            category = \"F&B\"\n",
    "\n",
    "        data[\"Company\"].append(company[\"name\"])\n",
    "        data[\"Website\"].append(company[\"url\"])\n",
    "        data[\"Relevant\"].append(\"Yes\")\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Manufacturer\"].append(manufacturer)\n",
    "        data[\"Brand\"].append(brand)\n",
    "        data[\"Distributor\"].append(distributor)\n",
    "        data[\"F&B\"].append(f_and_b)\n",
    "        data[\"Probiotics\"].append(probiotics)\n",
    "        data[\"Fortification\"].append(fortification)\n",
    "        data[\"Gut Health\"].append(gut_health)\n",
    "        data[\"Womens Health\"].append(womens_health)\n",
    "        data[\"Cognitive Health\"].append(cognitive_health)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company['name']}: {e}\")\n",
    "\n",
    "for company in companies:\n",
    "    scrape_website(company)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"test.csv\", index=False)\n",
    "driver.quit()\n",
    "print(\"Data scraping completed. Results saved to 'company_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping for nON-error Prone Wsbites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nestle...\n",
      "Accessing URL: https://www.nestle.com (Attempt 1)\n",
      "Processing Dr. Reddy's Laboratories...\n",
      "Accessing URL: https://www.drreddys.com (Attempt 1)\n",
      "Processing Coca...\n",
      "Accessing URL: https://www.coca-colacompany.com (Attempt 1)\n",
      "Processing Pfizer...\n",
      "Accessing URL: https://www.pfizer.com (Attempt 1)\n",
      "Processing PepsiCo...\n",
      "Accessing URL: https://www.pepsico.com (Attempt 1)\n",
      "Processing Johnson & Johnson...\n",
      "Accessing URL: https://www.jnj.com (Attempt 1)\n",
      "Processing Danone...\n",
      "Accessing URL: https://www.danone.com (Attempt 1)\n",
      "Processing General Mills...\n",
      "Accessing URL: https://www.generalmills.com (Attempt 1)\n",
      "Processing GlaxoSmithKline (GSK)...\n",
      "Accessing URL: https://www.gsk.com (Attempt 1)\n",
      "Processing Merck & Co....\n",
      "Accessing URL: https://www.merck.com (Attempt 1)\n",
      "Processing Unilever...\n",
      "Accessing URL: https://www.unilever.com (Attempt 1)\n",
      "Processing Roche...\n",
      "Accessing URL: https://www.roche.com (Attempt 1)\n",
      "Processing Nestle Waters...\n",
      "Accessing URL: https://www.nestlewaters.com (Attempt 1)\n",
      "Processing Sanofi...\n",
      "Accessing URL: https://www.sanofi.com (Attempt 1)\n",
      "Processing Mondelez International...\n",
      "Accessing URL: https://www.mondelezinternational.com (Attempt 1)\n",
      "Processing Novartis...\n",
      "Accessing URL: https://www.novartis.com (Attempt 1)\n",
      "Processing Kraft Heinz...\n",
      "Accessing URL: https://www.kraftheinzcompany.com (Attempt 1)\n",
      "Processing Tyson Foods...\n",
      "Accessing URL: https://www.tysonfoods.com (Attempt 1)\n",
      "Processing Teva Pharmaceuticals...\n",
      "Accessing URL: https://www.tevapharm.com (Attempt 1)\n",
      "Processing Mars Incorporated...\n",
      "Accessing URL: https://www.mars.com (Attempt 1)\n",
      "Processing Campbell Soup Company...\n",
      "Accessing URL: https://www.campbellsoupcompany.com (Attempt 1)\n",
      "Processing Amgen...\n",
      "Accessing URL: https://www.amgen.com (Attempt 1)\n",
      "Processing Conagra Brands...\n",
      "Accessing URL: https://www.conagrabrands.com (Attempt 1)\n",
      "Processing AstraZeneca...\n",
      "Accessing URL: https://www.astrazeneca.com (Attempt 1)\n",
      "Processing Molson Coors...\n",
      "Accessing URL: https://www.molsoncoors.com (Attempt 1)\n",
      "Processing Boehringer Ingelheim...\n",
      "Accessing URL: https://www.boehringeringelheim.com (Attempt 1)\n",
      "Processing AB InBev...\n",
      "Accessing URL: https://www.abinbev.com (Attempt 1)\n",
      "Processing BASF...\n",
      "Accessing URL: https://www.basf.com (Attempt 1)\n",
      "Processing Diageo...\n",
      "Accessing URL: https://www.diageo.com (Attempt 1)\n",
      "Processing Procter & Gamble (P&G)...\n",
      "Accessing URL: https://www.pg.com (Attempt 1)\n",
      "Processing Heineken...\n",
      "Accessing URL: https://www.theheinekencompany.com (Attempt 1)\n",
      "Processing Medtronic...\n",
      "Accessing URL: https://www.medtronic.com (Attempt 1)\n",
      "Processing McKesson...\n",
      "Accessing URL: https://www.mckesson.com (Attempt 1)\n",
      "Processing AmerisourceBergen...\n",
      "Accessing URL: https://www.amerisourcebergen.com (Attempt 1)\n",
      "Processing Cardinal Health...\n",
      "Accessing URL: https://www.cardinalhealth.com (Attempt 1)\n",
      "Processing Kellogs...\n",
      "Accessing URL: https://www.kellogs.com (Attempt 1)\n",
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\", encoding=\"ISO-8859-1\", on_bad_lines='skip')\n",
    "dataset = dataset.drop(columns=['Unnamed: 2'], errors='ignore')\n",
    "\n",
    "def ensure_url_scheme(df, url_column):\n",
    "    for index, row in df.iterrows():\n",
    "        url = row[url_column]\n",
    "        if not url.startswith('https://'):\n",
    "            url = 'https://' + url\n",
    "        row[url_column] = url\n",
    "\n",
    "ensure_url_scheme(dataset, 'url')\n",
    "\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    attempts = 3\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            print(f\"Accessing URL: {url} (Attempt {attempt + 1})\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "            brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "            distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "            relevant = \"Yes\"\n",
    "            category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "            \n",
    "            probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "            fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "            gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "            womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "            cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "            \n",
    "            return {\n",
    "                \"Company\": company[\"company_name\"],\n",
    "                \"Website\": url,\n",
    "                \"Relevant\": relevant,\n",
    "                \"Category\": category,\n",
    "                \"Manufacturer\": manufacturer,\n",
    "                \"Brand\": brand,\n",
    "                \"Distributor\": distributor,\n",
    "                \"F&B\": \"Yes\",\n",
    "                \"Probiotics\": probiotics,\n",
    "                \"Fortification\": fortification,\n",
    "                \"Gut Health\": gut_health,\n",
    "                \"Womens Health\": womens_health,\n",
    "                \"Cognitive Health\": cognitive_health\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 403:\n",
    "                print(f\"Access denied for {company['company_name']}: {e}. Retrying...\")\n",
    "            else:\n",
    "                print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, company in dataset.iterrows():\n",
    "    print(f\"Processing {company['company_name']}...\")\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "csv_file = 'company_data.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium for 403 Error sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed. Results saved to 'test.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "options = Options()\n",
    "options.headless = False\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "data = {\n",
    "    \"Company\": [],\n",
    "    \"Website\": [],\n",
    "    \"Relevant\": [],\n",
    "    \"Category\": [],\n",
    "    \"Manufacturer\": [],\n",
    "    \"Brand\": [],\n",
    "    \"Distributor\": [],\n",
    "    \"F&B\": [],\n",
    "    \"Probiotics\": [],\n",
    "    \"Fortification\": [],\n",
    "    \"Gut Health\": [],\n",
    "    \"Womens Health\": [],\n",
    "    \"Cognitive Health\": []\n",
    "}\n",
    "\n",
    "companies = [\n",
    "    {'name': 'bayer', 'url': 'https://www.bayer.com'},\n",
    "    {'name': 'lilly', 'url': 'https://www.lilly.com'},\n",
    "    {'name': 'abbvie', 'url': 'https://www.abbvie.com'},\n",
    "    {'name': 'medline', 'url': 'https://www.medline.com'},\n",
    "]\n",
    "\n",
    "def scrape_website(company):\n",
    "    try:\n",
    "        driver.get(company[\"url\"])\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        f_and_b = \"Yes\" if \"food\" in soup.text.lower() and \"beverage\" in soup.text.lower() else \"No\"\n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "\n",
    "        category = \"N/A\"\n",
    "        if manufacturer == \"Yes\":\n",
    "            category = \"Bulk (Manufacturer)\"\n",
    "        elif distributor == \"Yes\":\n",
    "            category = \"Bulk (Distributor)\"\n",
    "        elif brand == \"Yes\":\n",
    "            category = \"Brand\"\n",
    "        else:\n",
    "            category = \"F&B\"\n",
    "\n",
    "        data[\"Company\"].append(company[\"name\"])\n",
    "        data[\"Website\"].append(company[\"url\"])\n",
    "        data[\"Relevant\"].append(\"Yes\")\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Manufacturer\"].append(manufacturer)\n",
    "        data[\"Brand\"].append(brand)\n",
    "        data[\"Distributor\"].append(distributor)\n",
    "        data[\"F&B\"].append(f_and_b)\n",
    "        data[\"Probiotics\"].append(probiotics)\n",
    "        data[\"Fortification\"].append(fortification)\n",
    "        data[\"Gut Health\"].append(gut_health)\n",
    "        data[\"Womens Health\"].append(womens_health)\n",
    "        data[\"Cognitive Health\"].append(cognitive_health)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company['name']}: {e}\")\n",
    "\n",
    "for company in companies:\n",
    "    scrape_website(company)\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"test.csv\", index=False)\n",
    "driver.quit()\n",
    "print(\"Data scraping completed. Results saved to 'test.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Two result csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merging completed. Results saved to 'result.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading datasets\n",
    "company_data = pd.read_csv(\"company_data.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# mergin the datasets\n",
    "merged_data = pd.concat([company_data, test], ignore_index=True)\n",
    "\n",
    "# Save the mergred dataset to a new csv files\n",
    "merged_data.to_csv(\"FINAL-result.csv\", index=False)\n",
    "\n",
    "print(\"Data merging completed. Results saved to 'result.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
