{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For easily-scrappable company Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nestle...\n",
      "Accessing URL: https://www.nestle.com (Attempt 1)\n",
      "Processing Dr. Reddy's Laboratories...\n",
      "Accessing URL: https://www.drreddys.com (Attempt 1)\n",
      "Processing Coca...\n",
      "Accessing URL: https://www.coca-colacompany.com (Attempt 1)\n",
      "Processing Pfizer...\n",
      "Accessing URL: https://www.pfizer.com (Attempt 1)\n",
      "Processing PepsiCo...\n",
      "Accessing URL: https://www.pepsico.com (Attempt 1)\n",
      "Processing Johnson & Johnson...\n",
      "Accessing URL: https://www.jnj.com (Attempt 1)\n",
      "Processing Danone...\n",
      "Accessing URL: https://www.danone.com (Attempt 1)\n",
      "Processing General Mills...\n",
      "Accessing URL: https://www.generalmills.com (Attempt 1)\n",
      "Processing GlaxoSmithKline (GSK)...\n",
      "Accessing URL: https://www.gsk.com (Attempt 1)\n",
      "Processing Merck & Co....\n",
      "Accessing URL: https://www.merck.com (Attempt 1)\n",
      "Processing Unilever...\n",
      "Accessing URL: https://www.unilever.com (Attempt 1)\n",
      "Processing Roche...\n",
      "Accessing URL: https://www.roche.com (Attempt 1)\n",
      "Processing Nestle Waters...\n",
      "Accessing URL: https://www.nestlewaters.com (Attempt 1)\n",
      "Processing Sanofi...\n",
      "Accessing URL: https://www.sanofi.com (Attempt 1)\n",
      "Processing Mondelez International...\n",
      "Accessing URL: https://www.mondelezinternational.com (Attempt 1)\n",
      "Processing Novartis...\n",
      "Accessing URL: https://www.novartis.com (Attempt 1)\n",
      "Processing Kraft Heinz...\n",
      "Accessing URL: https://www.kraftheinzcompany.com (Attempt 1)\n",
      "Processing Tyson Foods...\n",
      "Accessing URL: https://www.tysonfoods.com (Attempt 1)\n",
      "Processing Teva Pharmaceuticals...\n",
      "Accessing URL: https://www.tevapharm.com (Attempt 1)\n",
      "Processing Mars Incorporated...\n",
      "Accessing URL: https://www.mars.com (Attempt 1)\n",
      "Processing Campbell Soup Company...\n",
      "Accessing URL: https://www.campbellsoupcompany.com (Attempt 1)\n",
      "Processing Amgen...\n",
      "Accessing URL: https://www.amgen.com (Attempt 1)\n",
      "Processing Conagra Brands...\n",
      "Accessing URL: https://www.conagrabrands.com (Attempt 1)\n",
      "Processing AstraZeneca...\n",
      "Accessing URL: https://www.astrazeneca.com (Attempt 1)\n",
      "Processing Molson Coors...\n",
      "Accessing URL: https://www.molsoncoors.com (Attempt 1)\n",
      "Processing Boehringer Ingelheim...\n",
      "Accessing URL: https://www.boehringeringelheim.com (Attempt 1)\n",
      "Processing AB InBev...\n",
      "Accessing URL: https://www.abinbev.com (Attempt 1)\n",
      "Processing BASF...\n",
      "Accessing URL: https://www.basf.com (Attempt 1)\n",
      "Processing Diageo...\n",
      "Accessing URL: https://www.diageo.com (Attempt 1)\n",
      "Processing Procter & Gamble (P&G)...\n",
      "Accessing URL: https://www.pg.com (Attempt 1)\n",
      "Processing Heineken...\n",
      "Accessing URL: https://www.theheinekencompany.com (Attempt 1)\n",
      "Processing Medtronic...\n",
      "Accessing URL: https://www.medtronic.com (Attempt 1)\n",
      "Processing McKesson...\n",
      "Accessing URL: https://www.mckesson.com (Attempt 1)\n",
      "Processing AmerisourceBergen...\n",
      "Accessing URL: https://www.amerisourcebergen.com (Attempt 1)\n",
      "Processing Cardinal Health...\n",
      "Accessing URL: https://www.cardinalhealth.com (Attempt 1)\n",
      "Processing Kellogs...\n",
      "Accessing URL: https://www.kellogs.com (Attempt 1)\n",
      "Data scraping completed. Results saved to 'company_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load the dataset with error handling for bad lines\n",
    "dataset = pd.read_csv(\"dataset.csv\", encoding=\"ISO-8859-1\", on_bad_lines='skip')\n",
    "dataset = dataset.drop(columns=['Unnamed: 2'], errors='ignore')\n",
    "\n",
    "# Function to ensure URLs have the 'http://' scheme\n",
    "def ensure_url_scheme(df, url_column):\n",
    "    for index, row in df.iterrows():\n",
    "        url = row[url_column]\n",
    "        if not url.startswith('https://') :\n",
    "            url = 'https://' + url\n",
    "        row[url_column] = url\n",
    "\n",
    "# Apply the function to the dataset\n",
    "ensure_url_scheme(dataset, 'url')\n",
    "\n",
    "# Function to scrape data from a company website\n",
    "def scrape_company_data(company):\n",
    "    url = company[\"url\"]\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    attempts = 3  # Number of attempts to fetch the URL\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            print(f\"Accessing URL: {url} (Attempt {attempt + 1})\")  # Debug print for URLs\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses (4xx, 5xx)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Placeholder for the actual scraping logic\n",
    "            manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "            brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "            distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "            relevant = \"Yes\"\n",
    "            category = \"Bulk (Manufacturer)\" if manufacturer == \"Yes\" else \"Bulk (Distributor)\" if distributor == \"Yes\" else \"Brand\"\n",
    "            \n",
    "            # Fill health segments based on specific keywords found on the website\n",
    "            probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "            fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "            gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "            womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "            cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "            \n",
    "            return {\n",
    "                \"Company\": company[\"company_name\"],\n",
    "                \"Website\": url,\n",
    "                \"Relevant\": relevant,\n",
    "                \"Category\": category,\n",
    "                \"Manufacturer\": manufacturer,\n",
    "                \"Brand\": brand,\n",
    "                \"Distributor\": distributor,\n",
    "                \"F&B\": \"Yes\",  # Assuming these are all F&B\n",
    "                \"Probiotics\": probiotics,\n",
    "                \"Fortification\": fortification,\n",
    "                \"Gut Health\": gut_health,\n",
    "                \"Womens Health\": womens_health,\n",
    "                \"Cognitive Health\": cognitive_health\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 403:\n",
    "                print(f\"Access denied for {company['company_name']}: {e}. Retrying...\")\n",
    "            else:\n",
    "                print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            time.sleep(1)  # Wait before retrying\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing {company['company_name']}: {e}\")\n",
    "            return None\n",
    "    return None  # Return None if all attempts fail\n",
    "\n",
    "# List to hold the results\n",
    "results = []\n",
    "\n",
    "# Scrape data for each company in the dataset\n",
    "for index, company in dataset.iterrows():\n",
    "    print(f\"Processing {company['company_name']}...\")  # Debug print for company being processed\n",
    "    data = scrape_company_data(company)\n",
    "    if data:\n",
    "        results.append(data)\n",
    "    \n",
    "    # Add a delay to avoid overwhelming the servers\n",
    "    # time.sleep(2)  # Uncomment this line to add a delay between requests\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV\n",
    "csv_file = 'company_data.csv'\n",
    "df.to_csv(csv_file, index=False)  # Write mode\n",
    "\n",
    "print(f\"Data scraping completed. Results saved to '{csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium for 403 Error sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed. Results saved to 'test.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set up the options for the Chrome browser\n",
    "options = Options()\n",
    "options.headless = False  # Change to True if you want to run headless\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "data = {\n",
    "    \"Company\": [],\n",
    "    \"Website\": [],\n",
    "    \"Relevant\": [],\n",
    "    \"Category\": [],\n",
    "    \"Manufacturer\": [],\n",
    "    \"Brand\": [],\n",
    "    \"Distributor\": [],\n",
    "    \"F&B\": [],\n",
    "    \"Probiotics\": [],\n",
    "    \"Fortification\": [],\n",
    "    \"Gut Health\": [],\n",
    "    \"Womens Health\": [],\n",
    "    \"Cognitive Health\": []\n",
    "}\n",
    "\n",
    "companies = [\n",
    "    {'name': 'bayer', 'url': 'https://www.bayer.com'},\n",
    "    {'name': 'lilly', 'url': 'https://www.lilly.com'},\n",
    "    {'name': 'abbvie', 'url': 'https://www.abbvie.com'},\n",
    "    {'name': 'medline', 'url': 'https://www.medline.com'},\n",
    "]\n",
    "\n",
    "# Function to scrape website data\n",
    "def scrape_website(company):\n",
    "    try:\n",
    "        driver.get(company[\"url\"])\n",
    "\n",
    "        # Use WebDriverWait to wait for the page to load (adjust timeout as necessary)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract relevant information\n",
    "        manufacturer = \"Yes\" if \"manufacturer\" in soup.text.lower() else \"No\"\n",
    "        brand = \"Yes\" if \"brand\" in soup.text.lower() else \"No\"\n",
    "        distributor = \"Yes\" if \"distributor\" in soup.text.lower() else \"No\"\n",
    "        f_and_b = \"Yes\" if \"food\" in soup.text.lower() and \"beverage\" in soup.text.lower() else \"No\"\n",
    "        probiotics = \"Yes\" if \"probiotic\" in soup.text.lower() else \"No\"\n",
    "        fortification = \"Yes\" if \"fortified\" in soup.text.lower() else \"No\"\n",
    "        gut_health = \"Yes\" if \"gut health\" in soup.text.lower() else \"No\"\n",
    "        womens_health = \"Yes\" if \"women's health\" in soup.text.lower() else \"No\"\n",
    "        cognitive_health = \"Yes\" if \"cognitive health\" in soup.text.lower() else \"No\"\n",
    "\n",
    "        # Determine category\n",
    "        category = \"N/A\"\n",
    "        if manufacturer == \"Yes\":\n",
    "            category = \"Bulk (Manufacturer)\"\n",
    "        elif distributor == \"Yes\":\n",
    "            category = \"Bulk (Distributor)\"\n",
    "        elif brand == \"Yes\":\n",
    "            category = \"Brand\"\n",
    "        else:\n",
    "            category = \"F&B\"\n",
    "\n",
    "        # Append data to the data structure\n",
    "        data[\"Company\"].append(company[\"name\"])\n",
    "        data[\"Website\"].append(company[\"url\"])\n",
    "        data[\"Relevant\"].append(\"Yes\")\n",
    "        data[\"Category\"].append(category)\n",
    "        data[\"Manufacturer\"].append(manufacturer)\n",
    "        data[\"Brand\"].append(brand)\n",
    "        data[\"Distributor\"].append(distributor)\n",
    "        data[\"F&B\"].append(f_and_b)\n",
    "        data[\"Probiotics\"].append(probiotics)\n",
    "        data[\"Fortification\"].append(fortification)\n",
    "        data[\"Gut Health\"].append(gut_health)\n",
    "        data[\"Womens Health\"].append(womens_health)\n",
    "        data[\"Cognitive Health\"].append(cognitive_health)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company['name']}: {e}\")\n",
    "\n",
    "# Scrape all company websites\n",
    "for company in companies:\n",
    "    scrape_website(company)\n",
    "    time.sleep(random.uniform(2, 5))  # Random sleep between requests\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"test.csv\", index=False)\n",
    "driver.quit()\n",
    "print(\"Data scraping completed. Results saved to 'test.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Two result csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merging completed. Results saved to 'result.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "company_data = pd.read_csv(\"company_data.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = pd.concat([company_data, test], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_data.to_excel(\"result.xlsx\", index=False)\n",
    "\n",
    "print(\"Data merging completed. Results saved to 'result.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
